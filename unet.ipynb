{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.models import *\n",
    "from keras.layers import Input, merge, Conv2D, MaxPooling2D, UpSampling2D, Dropout, Cropping2D, Convolution2D, core, BatchNormalization\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras.layers.core import Lambda\n",
    "import keras\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.backend import binary_crossentropy\n",
    "\n",
    "# config = tf.ConfigProto()\n",
    "# config.gpu_options.allocator_type ='BFC'\n",
    "# config.gpu_options.per_process_gpu_memory_fraction = 0.90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Creating training images...\n",
      "------------------------------\n",
      "306\n",
      "loading done\n",
      "Saving to .npy files done.\n",
      "------------------------------\n",
      "Creating test images...\n",
      "------------------------------\n",
      "77\n",
      "loading done\n",
      "Saving to imgs_test.npy files done.\n"
     ]
    }
   ],
   "source": [
    "%run 'data.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_batch(x, n_gpus, part):\n",
    "    \"\"\"\n",
    "    Divide the input batch into [n_gpus] slices, and obtain slice no.\n",
    "    [part].\n",
    "\n",
    "    i.e. if len(x)=10, then slice_batch(x, 2, 1) will return x[5:].\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    sh = K.shape(x)\n",
    "\n",
    "    L = sh[0] / n_gpus\n",
    "    L = tf.cast(L, tf.int32)\n",
    "\n",
    "    if part == n_gpus - 1:\n",
    "\n",
    "        return x[part*L:]\n",
    "\n",
    "    return x[part*L:int(part+1)*L]\n",
    "\n",
    "def to_multi_gpu(model, n_gpus=4):\n",
    "\n",
    "    \"\"\"Given a keras [model], return an equivalent model which parallelizes\n",
    "\n",
    "    the computation over [n_gpus] GPUs.\n",
    "\n",
    "\n",
    "\n",
    "    Each GPU gets a slice of the input batch, applies the model on that\n",
    "    slice\n",
    "\n",
    "    and later the outputs of the models are concatenated to a single\n",
    "    tensor,\n",
    "\n",
    "    hence the user sees a model that behaves the same as the original.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    with tf.device('/cpu:0'):\n",
    "\n",
    "        x = Input(model.input_shape[1:], name=model.input_names[0])\n",
    "\n",
    "\n",
    "    towers = []\n",
    "\n",
    "    for g in range(n_gpus):\n",
    "\n",
    "        with tf.device('/gpu:' + str(g)):\n",
    "\n",
    "            slice_g = Lambda(slice_batch, lambda shape: shape, arguments={'n_gpus':n_gpus, 'part':g})(x)\n",
    "\n",
    "            towers.append(model(slice_g))\n",
    "\n",
    "\n",
    "    with tf.device('/cpu:0'):\n",
    "\n",
    "        merged = merge(towers, mode='concat', concat_axis=0)\n",
    "\n",
    "    new_model = Model(input=[x], output=merged)\n",
    "    \n",
    "    funcType = type(model.save)\n",
    "    # monkeypatch the save to save just the underlying model\n",
    "    def new_save(self_,filepath, overwrite=True):\n",
    "        model.save(filepath, overwrite)\n",
    "    new_model.save=funcType(new_save, new_model)\n",
    "   \n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth = 1e-12\n",
    "\n",
    "def jaccard_coef(y_true, y_pred):\n",
    "    intersection = K.sum(y_true * y_pred, axis=[0, -1, -2])\n",
    "    sum_ = K.sum(y_true + y_pred, axis=[0, -1, -2])\n",
    "\n",
    "    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n",
    "\n",
    "    return K.mean(jac)\n",
    "\n",
    "\n",
    "def jaccard_coef_int(y_true, y_pred):\n",
    "    y_pred_pos = K.round(K.clip(y_pred, 0, 1))\n",
    "\n",
    "    intersection = K.sum(y_true * y_pred_pos, axis=[0, -1, -2])\n",
    "    sum_ = K.sum(y_true + y_pred_pos, axis=[0, -1, -2])\n",
    "\n",
    "    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n",
    "\n",
    "    return K.mean(jac)\n",
    "\n",
    "\n",
    "def jaccard_coef_loss(y_true, y_pred):\n",
    "    return -K.log(jaccard_coef(y_true, y_pred)) + binary_crossentropy(y_pred, y_true)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myUnet(object):\n",
    "\n",
    "    def __init__(self, tp = 'buildings', img_rows = 512, img_cols = 512):\n",
    "\n",
    "        self.type = tp\n",
    "        self.img_rows = img_rows\n",
    "        self.img_cols = img_cols\n",
    "\n",
    "    def load_data(self):\n",
    "\n",
    "        mydata = dataProcess(self.img_rows, self.img_cols)\n",
    "        imgs_train, imgs_mask_train = mydata.load_train_data(self.type)\n",
    "        imgs_test = mydata.load_test_data()\n",
    "        return imgs_train, imgs_mask_train, imgs_test\n",
    "\n",
    "    #Define the neural network\n",
    "    def get_unet(self):\n",
    "        inputs = Input((512, 512, 1))\n",
    "        #\n",
    "        conv1 = Convolution2D(32, 3, 3, border_mode='same', kernel_initializer = 'he_normal')(inputs)\n",
    "        conv1 = BatchNormalization(mode=0, axis=1)(conv1)\n",
    "        conv1 = keras.layers.advanced_activations.ELU()(conv1)\n",
    "        conv1 = Convolution2D(32, 3, 3, border_mode='same', kernel_initializer = 'he_normal')(conv1)\n",
    "        conv1 = BatchNormalization(mode=0, axis=1)(conv1)\n",
    "        conv1 = keras.layers.advanced_activations.ELU()(conv1)\n",
    "        pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "        #\n",
    "        conv2 = Convolution2D(64, 3, 3, border_mode='same', kernel_initializer = 'he_normal')(pool1)\n",
    "        conv2 = BatchNormalization(mode=0, axis=1)(conv2)\n",
    "        conv2 = keras.layers.advanced_activations.ELU()(conv2)\n",
    "        conv2 = Convolution2D(64, 3, 3, activation='relu', border_mode='same', kernel_initializer = 'he_normal')(conv2)\n",
    "        conv2 = BatchNormalization(mode=0, axis=1)(conv2)\n",
    "        conv2 = keras.layers.advanced_activations.ELU()(conv2)\n",
    "        pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "        #\n",
    "        conv3 = Convolution2D(128, 3, 3, border_mode='same', kernel_initializer = 'he_normal')(pool2)\n",
    "        conv3 = BatchNormalization(mode=0, axis=1)(conv3)\n",
    "        conv3 = keras.layers.advanced_activations.ELU()(conv3)\n",
    "        conv3 = Convolution2D(128, 3, 3, border_mode='same', kernel_initializer = 'he_normal')(conv3)\n",
    "        conv3 = BatchNormalization(mode=0, axis=1)(conv3)\n",
    "        conv3 = keras.layers.advanced_activations.ELU()(conv3)\n",
    "        pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "        #\n",
    "        conv4 = Convolution2D(256, 3, 3, border_mode='same', kernel_initializer = 'he_normal')(pool3)\n",
    "        conv4 = BatchNormalization(mode=0, axis=1)(conv4)\n",
    "        conv4 = keras.layers.advanced_activations.ELU()(conv4)\n",
    "        conv4 = Convolution2D(256, 3, 3, border_mode='same', kernel_initializer = 'he_normal')(conv4)\n",
    "        conv4 = BatchNormalization(mode=0, axis=1)(conv4)\n",
    "        conv4 = keras.layers.advanced_activations.ELU()(conv4)\n",
    "#         drop4 = Dropout(0.5)(conv?4)\n",
    "        pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "        #\n",
    "        conv5 = Convolution2D(512, 3, 3, border_mode='same', kernel_initializer='he_normal')(pool4)\n",
    "        conv5 = BatchNormalization(mode=0, axis=1)(conv5)\n",
    "        conv5 = keras.layers.advanced_activations.ELU()(conv5)\n",
    "        conv5 = Convolution2D(512, 3, 3, border_mode='same', kernel_initializer='he_normal')(conv5)\n",
    "        conv5 = BatchNormalization(mode=0, axis=1)(conv5)\n",
    "        conv5 = keras.layers.advanced_activations.ELU()(conv5)\n",
    "#         drop5 = Dropout(0.5)(conv5)\n",
    "        #\n",
    "        up1 = merge([UpSampling2D(size=(2, 2))(conv5), conv4], mode='concat', concat_axis=-1)\n",
    "        conv6 = Convolution2D(256, 3, 3, border_mode='same', kernel_initializer = 'he_normal')(up1)\n",
    "        conv6 = BatchNormalization(mode=0, axis=1)(conv6)\n",
    "        conv6 = keras.layers.advanced_activations.ELU()(conv6)\n",
    "        conv6 = Convolution2D(256, 3, 3, border_mode='same', kernel_initializer = 'he_normal')(conv6)\n",
    "        conv6 = BatchNormalization(mode=0, axis=1)(conv6)\n",
    "        conv6 = keras.layers.advanced_activations.ELU()(conv6)\n",
    "        #\n",
    "        up2 = merge([UpSampling2D(size=(2, 2))(conv6), conv3], mode='concat', concat_axis=-1)\n",
    "        conv7 = Convolution2D(128, 3, 3, border_mode='same', kernel_initializer = 'he_normal')(up2)\n",
    "        conv7 = BatchNormalization(mode=0, axis=1)(conv7)\n",
    "        conv7 = keras.layers.advanced_activations.ELU()(conv7)\n",
    "        conv7 = Convolution2D(128, 3, 3, border_mode='same', kernel_initializer = 'he_normal')(conv7)\n",
    "        conv7 = BatchNormalization(mode=0, axis=1)(conv7)\n",
    "        conv7 = keras.layers.advanced_activations.ELU()(conv7)\n",
    "        #\n",
    "        up3 = merge([UpSampling2D(size=(2, 2))(conv7), conv2], mode='concat', concat_axis=-1)\n",
    "        conv8 = Convolution2D(64, 3, 3, border_mode='same', kernel_initializer='he_normal')(up3)\n",
    "        conv8 = BatchNormalization(mode=0, axis=1)(conv8)\n",
    "        conv8 = keras.layers.advanced_activations.ELU()(conv8)\n",
    "        conv8 = Convolution2D(64, 3, 3, border_mode='same', kernel_initializer='he_normal')(conv8)\n",
    "        conv8 = BatchNormalization(mode=0, axis=1)(conv8)\n",
    "        conv8 = keras.layers.advanced_activations.ELU()(conv8)\n",
    "        #\n",
    "        up4 = merge([UpSampling2D(size=(2, 2))(conv8), conv1], mode='concat', concat_axis=-1)\n",
    "        conv9 = Convolution2D(32, 3, 3, border_mode='same', kernel_initializer = 'he_normal')(up4)\n",
    "        conv9 = BatchNormalization(mode=0, axis=1)(conv9)\n",
    "        conv9 = keras.layers.advanced_activations.ELU()(conv9)\n",
    "        conv9 = Convolution2D(32, 3, 3, border_mode='same', kernel_initializer = 'he_normal')(conv9)\n",
    "        conv9 = BatchNormalization(mode=0, axis=1)(conv9)\n",
    "        conv9 = keras.layers.advanced_activations.ELU()(conv9)\n",
    "        conv9 = Convolution2D(2, 3, 3, border_mode='same', kernel_initializer = 'he_normal')(conv9)\n",
    "        #\n",
    "        conv10 = Convolution2D(1, 1, 1, activation='sigmoid')(conv9)\n",
    "\n",
    "        model = Model(input=inputs, output=conv10)\n",
    "\n",
    "        model.compile(optimizer= Nadam(lr = 1e-3), loss=jaccard_coef_loss, metrics=['binary_crossentropy', jaccard_coef_int])\n",
    "        model = to_multi_gpu(model)\n",
    "        model.compile(optimizer= Nadam(lr = 1e-3), loss=jaccard_coef_loss, metrics=['binary_crossentropy', jaccard_coef_int])\n",
    "\n",
    "        return model\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "\n",
    "        print(\"loading data\")\n",
    "        imgs_train, imgs_mask_train, imgs_test = self.load_data()\n",
    "        print(\"loading data done\")\n",
    "        model = self.get_unet()\n",
    "        print(\"got unet\")\n",
    "\n",
    "        model_checkpoint = ModelCheckpoint('unet.hdf5', monitor='loss',verbose=1, save_best_only=True, mode='min')\n",
    "        print('Fitting model...')\n",
    "#         model.fit(imgs_train, imgs_mask_train, batch_size=8, nb_epoch=10, verbose=1, validation_split=0.2, shuffle=True, callbacks=[model_checkpoint])\n",
    "        model.fit(imgs_train, imgs_mask_train, batch_size=8, nb_epoch=50, verbose=1, validation_split=0.3, shuffle=True, callbacks=[model_checkpoint])\n",
    "\n",
    "        \n",
    "        print('predict test data')\n",
    "        imgs_mask_test = model.predict(imgs_test, batch_size=8, verbose=1)\n",
    "        if self.type == 'buildings':\n",
    "            np.save('../results/imgs_mask_test.npy', imgs_mask_test)\n",
    "        elif self.type == 'cars':\n",
    "            np.save('../results/imgs_mask_test_cars.npy', imgs_mask_test)\n",
    "\n",
    "    def save_img(self):\n",
    "\n",
    "        print(\"array to image\")\n",
    "        if self.tp == 'buildings':\n",
    "            imgs = np.load('../results/imgs_mask_test.npy')\n",
    "        elif self.tp == 'cars':\n",
    "            imgs = np.load('../results/imgs_mask_test_cars.npy')\n",
    "        for i in range(imgs.shape[0]):\n",
    "            img = imgs[i]\n",
    "            img = array_to_img(img)\n",
    "            img.save(\"../results/%d.jpg\"%(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data\n",
      "------------------------------\n",
      "load train images...\n",
      "------------------------------\n",
      "------------------------------\n",
      "load test images...\n",
      "------------------------------\n",
      "loading data done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:20: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\")`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:21: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(axis=1)`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:23: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\")`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:24: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(axis=1)`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:28: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\")`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:29: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(axis=1)`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\", kernel_initializer=\"he_normal\")`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:32: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(axis=1)`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:36: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\")`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:37: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(axis=1)`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:39: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\")`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:40: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(axis=1)`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:44: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\")`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:45: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(axis=1)`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:47: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\")`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:48: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(axis=1)`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:53: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\")`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:54: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(axis=1)`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:56: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\")`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:57: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(axis=1)`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:61: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/usr/lib/python3.5/site-packages/keras/legacy/layers.py:458: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:62: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\")`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:63: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(axis=1)`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:65: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\")`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:66: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(axis=1)`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:69: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:70: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\")`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:71: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(axis=1)`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:73: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\")`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:74: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(axis=1)`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:77: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:78: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\")`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:79: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(axis=1)`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:81: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\")`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:82: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(axis=1)`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:85: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:86: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\")`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:87: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(axis=1)`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:89: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\")`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:90: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(axis=1)`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:92: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(2, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\")`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:94: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(1, (1, 1), activation=\"sigmoid\")`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:96: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"co...)`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:57: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:59: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"me...)`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:116: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got unet\n",
      "Fitting model...\n",
      "Train on 214 samples, validate on 92 samples\n",
      "Epoch 1/50\n",
      "208/214 [============================>.] - ETA: 1s - loss: 5.4744 - binary_crossentropy: 0.5168 - jaccard_coef_int: 0.0991Epoch 00001: loss improved from inf to 5.46076, saving model to unet.hdf5\n",
      "214/214 [==============================] - 65s 303ms/step - loss: 5.4608 - binary_crossentropy: 0.5198 - jaccard_coef_int: 0.1002 - val_loss: 9.7585 - val_binary_crossentropy: 1.7026 - val_jaccard_coef_int: 0.0224\n",
      "Epoch 2/50\n",
      "208/214 [============================>.] - ETA: 0s - loss: 4.0378 - binary_crossentropy: 0.6809 - jaccard_coef_int: 0.2041Epoch 00002: loss improved from 5.46076 to 4.04784, saving model to unet.hdf5\n",
      "214/214 [==============================] - 41s 190ms/step - loss: 4.0478 - binary_crossentropy: 0.6757 - jaccard_coef_int: 0.2007 - val_loss: 5.9909 - val_binary_crossentropy: 1.1746 - val_jaccard_coef_int: 0.0414\n",
      "Epoch 3/50\n",
      "208/214 [============================>.] - ETA: 0s - loss: 3.9609 - binary_crossentropy: 0.8090 - jaccard_coef_int: 0.2061Epoch 00003: loss improved from 4.04784 to 3.94383, saving model to unet.hdf5\n",
      "214/214 [==============================] - 40s 187ms/step - loss: 3.9438 - binary_crossentropy: 0.8064 - jaccard_coef_int: 0.2079 - val_loss: 3.6266 - val_binary_crossentropy: 0.7250 - val_jaccard_coef_int: 0.1948\n",
      "Epoch 4/50\n",
      "208/214 [============================>.] - ETA: 0s - loss: 3.8818 - binary_crossentropy: 0.8760 - jaccard_coef_int: 0.2176Epoch 00004: loss improved from 3.94383 to 3.89344, saving model to unet.hdf5\n",
      "214/214 [==============================] - 41s 192ms/step - loss: 3.8934 - binary_crossentropy: 0.8803 - jaccard_coef_int: 0.2157 - val_loss: 3.3808 - val_binary_crossentropy: 0.7257 - val_jaccard_coef_int: 0.2458\n",
      "Epoch 5/50\n",
      "208/214 [============================>.] - ETA: 0s - loss: 3.8394 - binary_crossentropy: 0.9594 - jaccard_coef_int: 0.2150Epoch 00005: loss improved from 3.89344 to 3.82459, saving model to unet.hdf5\n",
      "214/214 [==============================] - 41s 191ms/step - loss: 3.8246 - binary_crossentropy: 0.9614 - jaccard_coef_int: 0.2195 - val_loss: 5.9184 - val_binary_crossentropy: 1.3825 - val_jaccard_coef_int: 0.2158\n",
      "Epoch 6/50\n",
      "208/214 [============================>.] - ETA: 0s - loss: 3.7476 - binary_crossentropy: 0.9913 - jaccard_coef_int: 0.2302Epoch 00006: loss improved from 3.82459 to 3.76723, saving model to unet.hdf5\n",
      "214/214 [==============================] - 42s 194ms/step - loss: 3.7672 - binary_crossentropy: 0.9978 - jaccard_coef_int: 0.2280 - val_loss: 3.1988 - val_binary_crossentropy: 0.7713 - val_jaccard_coef_int: 0.2754\n",
      "Epoch 7/50\n",
      "208/214 [============================>.] - ETA: 0s - loss: 3.6867 - binary_crossentropy: 0.9912 - jaccard_coef_int: 0.2437Epoch 00007: loss improved from 3.76723 to 3.68708, saving model to unet.hdf5\n",
      "214/214 [==============================] - 41s 189ms/step - loss: 3.6871 - binary_crossentropy: 0.9896 - jaccard_coef_int: 0.2420 - val_loss: 3.3883 - val_binary_crossentropy: 0.8404 - val_jaccard_coef_int: 0.2284\n",
      "Epoch 8/50\n",
      "208/214 [============================>.] - ETA: 0s - loss: 3.6722 - binary_crossentropy: 1.0225 - jaccard_coef_int: 0.2480Epoch 00008: loss improved from 3.68708 to 3.63028, saving model to unet.hdf5\n",
      "214/214 [==============================] - 40s 189ms/step - loss: 3.6303 - binary_crossentropy: 1.0074 - jaccard_coef_int: 0.2515 - val_loss: 3.3177 - val_binary_crossentropy: 0.8084 - val_jaccard_coef_int: 0.2891\n",
      "Epoch 9/50\n",
      "208/214 [============================>.] - ETA: 0s - loss: 3.4360 - binary_crossentropy: 0.9875 - jaccard_coef_int: 0.2747Epoch 00009: loss improved from 3.63028 to 3.39265, saving model to unet.hdf5\n",
      "214/214 [==============================] - 41s 192ms/step - loss: 3.3927 - binary_crossentropy: 0.9731 - jaccard_coef_int: 0.2812 - val_loss: 3.0200 - val_binary_crossentropy: 0.8633 - val_jaccard_coef_int: 0.3046\n",
      "Epoch 10/50\n",
      "208/214 [============================>.] - ETA: 0s - loss: 3.3998 - binary_crossentropy: 1.0386 - jaccard_coef_int: 0.2805Epoch 00010: loss improved from 3.39265 to 3.38881, saving model to unet.hdf5\n",
      "214/214 [==============================] - 41s 193ms/step - loss: 3.3888 - binary_crossentropy: 1.0340 - jaccard_coef_int: 0.2804 - val_loss: 3.4846 - val_binary_crossentropy: 1.0199 - val_jaccard_coef_int: 0.2118\n",
      "Epoch 11/50\n",
      "208/214 [============================>.] - ETA: 0s - loss: 3.2145 - binary_crossentropy: 1.0046 - jaccard_coef_int: 0.3050Epoch 00011: loss improved from 3.38881 to 3.21733, saving model to unet.hdf5\n",
      "214/214 [==============================] - 41s 191ms/step - loss: 3.2173 - binary_crossentropy: 1.0029 - jaccard_coef_int: 0.3031 - val_loss: 2.9277 - val_binary_crossentropy: 0.8454 - val_jaccard_coef_int: 0.3085\n",
      "Epoch 12/50\n",
      "208/214 [============================>.] - ETA: 0s - loss: 3.1736 - binary_crossentropy: 1.0255 - jaccard_coef_int: 0.3148Epoch 00012: loss improved from 3.21733 to 3.14488, saving model to unet.hdf5\n",
      "214/214 [==============================] - 40s 187ms/step - loss: 3.1449 - binary_crossentropy: 1.0130 - jaccard_coef_int: 0.3166 - val_loss: 3.2955 - val_binary_crossentropy: 0.9993 - val_jaccard_coef_int: 0.2250\n",
      "Epoch 13/50\n",
      "208/214 [============================>.] - ETA: 0s - loss: 3.0324 - binary_crossentropy: 1.0083 - jaccard_coef_int: 0.3300Epoch 00013: loss improved from 3.14488 to 3.06059, saving model to unet.hdf5\n",
      "214/214 [==============================] - 40s 188ms/step - loss: 3.0606 - binary_crossentropy: 1.0218 - jaccard_coef_int: 0.3269 - val_loss: 2.9321 - val_binary_crossentropy: 0.9065 - val_jaccard_coef_int: 0.2703\n",
      "Epoch 14/50\n",
      "208/214 [============================>.] - ETA: 0s - loss: 3.0753 - binary_crossentropy: 1.0683 - jaccard_coef_int: 0.3315Epoch 00014: loss did not improve\n",
      "214/214 [==============================] - 41s 191ms/step - loss: 3.0795 - binary_crossentropy: 1.0660 - jaccard_coef_int: 0.3283 - val_loss: 3.3697 - val_binary_crossentropy: 1.0550 - val_jaccard_coef_int: 0.2043\n",
      "Epoch 15/50\n",
      "208/214 [============================>.] - ETA: 0s - loss: 3.2514 - binary_crossentropy: 1.1603 - jaccard_coef_int: 0.3189Epoch 00015: loss did not improve\n",
      "214/214 [==============================] - 41s 192ms/step - loss: 3.2444 - binary_crossentropy: 1.1567 - jaccard_coef_int: 0.3173 - val_loss: 3.1772 - val_binary_crossentropy: 0.8905 - val_jaccard_coef_int: 0.2513\n",
      "Epoch 16/50\n",
      "208/214 [============================>.] - ETA: 0s - loss: 3.0295 - binary_crossentropy: 1.1164 - jaccard_coef_int: 0.3367Epoch 00016: loss improved from 3.06059 to 3.01501, saving model to unet.hdf5\n",
      "214/214 [==============================] - 41s 192ms/step - loss: 3.0150 - binary_crossentropy: 1.1106 - jaccard_coef_int: 0.3382 - val_loss: 2.6786 - val_binary_crossentropy: 0.9555 - val_jaccard_coef_int: 0.3153\n",
      "Epoch 17/50\n",
      "208/214 [============================>.] - ETA: 0s - loss: 2.9111 - binary_crossentropy: 1.0779 - jaccard_coef_int: 0.3490Epoch 00017: loss improved from 3.01501 to 2.90410, saving model to unet.hdf5\n",
      "214/214 [==============================] - 41s 192ms/step - loss: 2.9041 - binary_crossentropy: 1.0756 - jaccard_coef_int: 0.3509 - val_loss: 3.1376 - val_binary_crossentropy: 1.0533 - val_jaccard_coef_int: 0.2350\n",
      "Epoch 18/50\n",
      "208/214 [============================>.] - ETA: 0s - loss: 2.9726 - binary_crossentropy: 1.0985 - jaccard_coef_int: 0.3546Epoch 00018: loss did not improve\n",
      "214/214 [==============================] - 41s 192ms/step - loss: 2.9779 - binary_crossentropy: 1.1046 - jaccard_coef_int: 0.3557 - val_loss: 2.4065 - val_binary_crossentropy: 0.9934 - val_jaccard_coef_int: 0.3637\n",
      "Epoch 19/50\n",
      "208/214 [============================>.] - ETA: 0s - loss: 2.7889 - binary_crossentropy: 1.0082 - jaccard_coef_int: 0.3770Epoch 00019: loss improved from 2.90410 to 2.77991, saving model to unet.hdf5\n",
      "214/214 [==============================] - 40s 189ms/step - loss: 2.7799 - binary_crossentropy: 1.0055 - jaccard_coef_int: 0.3775 - val_loss: 2.8409 - val_binary_crossentropy: 0.9758 - val_jaccard_coef_int: 0.2757\n",
      "Epoch 20/50\n",
      "208/214 [============================>.] - ETA: 0s - loss: 2.5088 - binary_crossentropy: 0.9722 - jaccard_coef_int: 0.4225Epoch 00020: loss improved from 2.77991 to 2.49649, saving model to unet.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "214/214 [==============================] - 41s 192ms/step - loss: 2.4965 - binary_crossentropy: 0.9670 - jaccard_coef_int: 0.4249 - val_loss: 2.0477 - val_binary_crossentropy: 0.7159 - val_jaccard_coef_int: 0.4530\n",
      "Epoch 21/50\n",
      "208/214 [============================>.] - ETA: 0s - loss: 2.5560 - binary_crossentropy: 0.9938 - jaccard_coef_int: 0.4136Epoch 00021: loss did not improve\n",
      "214/214 [==============================] - 41s 191ms/step - loss: 2.5292 - binary_crossentropy: 0.9834 - jaccard_coef_int: 0.4177 - val_loss: 2.2434 - val_binary_crossentropy: 0.7012 - val_jaccard_coef_int: 0.3989\n",
      "Epoch 22/50\n",
      "208/214 [============================>.] - ETA: 0s - loss: 2.3569 - binary_crossentropy: 0.9549 - jaccard_coef_int: 0.4534Epoch 00022: loss improved from 2.49649 to 2.36936, saving model to unet.hdf5\n",
      "214/214 [==============================] - 41s 191ms/step - loss: 2.3694 - binary_crossentropy: 0.9640 - jaccard_coef_int: 0.4517 - val_loss: 2.1638 - val_binary_crossentropy: 0.7433 - val_jaccard_coef_int: 0.4243\n",
      "Epoch 23/50\n",
      "208/214 [============================>.] - ETA: 0s - loss: 2.6599 - binary_crossentropy: 1.0907 - jaccard_coef_int: 0.4040Epoch 00023: loss did not improve\n",
      "214/214 [==============================] - 41s 193ms/step - loss: 2.7021 - binary_crossentropy: 1.0989 - jaccard_coef_int: 0.3958 - val_loss: 8.2181 - val_binary_crossentropy: 2.4910 - val_jaccard_coef_int: 0.1889\n",
      "Epoch 24/50\n",
      "208/214 [============================>.] - ETA: 0s - loss: 2.5612 - binary_crossentropy: 1.0250 - jaccard_coef_int: 0.4186Epoch 00024: loss did not improve\n",
      "214/214 [==============================] - 40s 189ms/step - loss: 2.5482 - binary_crossentropy: 1.0186 - jaccard_coef_int: 0.4190 - val_loss: 2.9453 - val_binary_crossentropy: 1.1384 - val_jaccard_coef_int: 0.2630\n",
      "Epoch 25/50\n",
      "208/214 [============================>.] - ETA: 0s - loss: 2.2817 - binary_crossentropy: 0.9426 - jaccard_coef_int: 0.4602Epoch 00025: loss improved from 2.36936 to 2.28012, saving model to unet.hdf5\n",
      "214/214 [==============================] - 41s 192ms/step - loss: 2.2801 - binary_crossentropy: 0.9432 - jaccard_coef_int: 0.4618 - val_loss: 1.9625 - val_binary_crossentropy: 0.7106 - val_jaccard_coef_int: 0.4674\n",
      "Epoch 26/50\n",
      "208/214 [============================>.] - ETA: 0s - loss: 2.0029 - binary_crossentropy: 0.8552 - jaccard_coef_int: 0.5198Epoch 00026: loss improved from 2.28012 to 2.01699, saving model to unet.hdf5\n",
      "214/214 [==============================] - 41s 192ms/step - loss: 2.0170 - binary_crossentropy: 0.8629 - jaccard_coef_int: 0.5171 - val_loss: 1.9251 - val_binary_crossentropy: 0.7050 - val_jaccard_coef_int: 0.4680\n",
      "Epoch 27/50\n",
      "208/214 [============================>.] - ETA: 0s - loss: 2.0819 - binary_crossentropy: 0.8964 - jaccard_coef_int: 0.5004Epoch 00027: loss did not improve\n",
      "214/214 [==============================] - 41s 192ms/step - loss: 2.0844 - binary_crossentropy: 0.8996 - jaccard_coef_int: 0.5008 - val_loss: 2.0421 - val_binary_crossentropy: 0.7466 - val_jaccard_coef_int: 0.4379\n",
      "Epoch 28/50\n",
      "208/214 [============================>.] - ETA: 0s - loss: 1.9787 - binary_crossentropy: 0.8622 - jaccard_coef_int: 0.5252Epoch 00028: loss improved from 2.01699 to 1.97940, saving model to unet.hdf5\n",
      "214/214 [==============================] - 41s 192ms/step - loss: 1.9794 - binary_crossentropy: 0.8629 - jaccard_coef_int: 0.5246 - val_loss: 1.7608 - val_binary_crossentropy: 0.6867 - val_jaccard_coef_int: 0.5276\n",
      "Epoch 29/50\n",
      "208/214 [============================>.] - ETA: 0s - loss: 1.9743 - binary_crossentropy: 0.8879 - jaccard_coef_int: 0.5257Epoch 00029: loss improved from 1.97940 to 1.96307, saving model to unet.hdf5\n",
      "214/214 [==============================] - 40s 187ms/step - loss: 1.9631 - binary_crossentropy: 0.8804 - jaccard_coef_int: 0.5258 - val_loss: 1.8477 - val_binary_crossentropy: 0.7161 - val_jaccard_coef_int: 0.4968\n",
      "Epoch 30/50\n",
      "208/214 [============================>.] - ETA: 0s - loss: 1.9450 - binary_crossentropy: 0.8787 - jaccard_coef_int: 0.5250Epoch 00030: loss improved from 1.96307 to 1.94264, saving model to unet.hdf5\n",
      "214/214 [==============================] - 41s 192ms/step - loss: 1.9426 - binary_crossentropy: 0.8788 - jaccard_coef_int: 0.5275 - val_loss: 1.8048 - val_binary_crossentropy: 0.7136 - val_jaccard_coef_int: 0.5019\n",
      "Epoch 31/50\n",
      "208/214 [============================>.] - ETA: 0s - loss: 1.8596 - binary_crossentropy: 0.8533 - jaccard_coef_int: 0.5440Epoch 00031: loss improved from 1.94264 to 1.86715, saving model to unet.hdf5\n",
      "214/214 [==============================] - 41s 194ms/step - loss: 1.8672 - binary_crossentropy: 0.8587 - jaccard_coef_int: 0.5429 - val_loss: 1.7794 - val_binary_crossentropy: 0.7254 - val_jaccard_coef_int: 0.5252\n",
      "Epoch 32/50\n",
      "208/214 [============================>.] - ETA: 0s - loss: 1.9290 - binary_crossentropy: 0.8915 - jaccard_coef_int: 0.5316Epoch 00032: loss did not improve\n",
      "214/214 [==============================] - 40s 187ms/step - loss: 1.9286 - binary_crossentropy: 0.8911 - jaccard_coef_int: 0.5341 - val_loss: 1.8336 - val_binary_crossentropy: 0.7723 - val_jaccard_coef_int: 0.5169\n",
      "Epoch 33/50\n",
      "208/214 [============================>.] - ETA: 0s - loss: 1.8409 - binary_crossentropy: 0.8586 - jaccard_coef_int: 0.5547Epoch 00033: loss improved from 1.86715 to 1.83401, saving model to unet.hdf5\n",
      "214/214 [==============================] - 40s 189ms/step - loss: 1.8340 - binary_crossentropy: 0.8528 - jaccard_coef_int: 0.5538 - val_loss: 1.7297 - val_binary_crossentropy: 0.6380 - val_jaccard_coef_int: 0.5205\n",
      "Epoch 34/50\n",
      "208/214 [============================>.] - ETA: 0s - loss: 1.8464 - binary_crossentropy: 0.8541 - jaccard_coef_int: 0.5482Epoch 00034: loss improved from 1.83401 to 1.83000, saving model to unet.hdf5\n",
      "214/214 [==============================] - 41s 191ms/step - loss: 1.8300 - binary_crossentropy: 0.8429 - jaccard_coef_int: 0.5487 - val_loss: 1.8499 - val_binary_crossentropy: 0.7277 - val_jaccard_coef_int: 0.4872\n",
      "Epoch 35/50\n",
      "208/214 [============================>.] - ETA: 0s - loss: 2.0697 - binary_crossentropy: 0.9696 - jaccard_coef_int: 0.5164Epoch 00035: loss did not improve\n",
      "214/214 [==============================] - 41s 191ms/step - loss: 2.0517 - binary_crossentropy: 0.9561 - jaccard_coef_int: 0.5152 - val_loss: 1.8982 - val_binary_crossentropy: 0.8553 - val_jaccard_coef_int: 0.4824\n",
      "Epoch 36/50\n",
      "208/214 [============================>.] - ETA: 0s - loss: 1.7803 - binary_crossentropy: 0.8399 - jaccard_coef_int: 0.5643Epoch 00036: loss improved from 1.83000 to 1.79618, saving model to unet.hdf5\n",
      "214/214 [==============================] - 40s 185ms/step - loss: 1.7962 - binary_crossentropy: 0.8467 - jaccard_coef_int: 0.5591 - val_loss: 2.5872 - val_binary_crossentropy: 1.2315 - val_jaccard_coef_int: 0.4400\n",
      "Epoch 37/50\n",
      "208/214 [============================>.] - ETA: 0s - loss: 1.6860 - binary_crossentropy: 0.7912 - jaccard_coef_int: 0.5773Epoch 00037: loss improved from 1.79618 to 1.69407, saving model to unet.hdf5\n",
      "214/214 [==============================] - 42s 195ms/step - loss: 1.6941 - binary_crossentropy: 0.7989 - jaccard_coef_int: 0.5781 - val_loss: 1.9930 - val_binary_crossentropy: 0.8754 - val_jaccard_coef_int: 0.5107\n",
      "Epoch 38/50\n",
      "208/214 [============================>.] - ETA: 0s - loss: 1.7777 - binary_crossentropy: 0.8219 - jaccard_coef_int: 0.5600Epoch 00038: loss did not improve\n",
      "214/214 [==============================] - 41s 189ms/step - loss: 1.7833 - binary_crossentropy: 0.8272 - jaccard_coef_int: 0.5607 - val_loss: 2.0086 - val_binary_crossentropy: 0.8888 - val_jaccard_coef_int: 0.4954\n",
      "Epoch 39/50\n",
      "208/214 [============================>.] - ETA: 0s - loss: 1.6542 - binary_crossentropy: 0.7905 - jaccard_coef_int: 0.5876Epoch 00039: loss improved from 1.69407 to 1.65692, saving model to unet.hdf5\n",
      "214/214 [==============================] - 41s 192ms/step - loss: 1.6569 - binary_crossentropy: 0.7921 - jaccard_coef_int: 0.5878 - val_loss: 1.8099 - val_binary_crossentropy: 0.6893 - val_jaccard_coef_int: 0.4867\n",
      "Epoch 40/50\n",
      "208/214 [============================>.] - ETA: 0s - loss: 1.6259 - binary_crossentropy: 0.7840 - jaccard_coef_int: 0.5972Epoch 00040: loss improved from 1.65692 to 1.62773, saving model to unet.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "214/214 [==============================] - 41s 192ms/step - loss: 1.6277 - binary_crossentropy: 0.7859 - jaccard_coef_int: 0.5974 - val_loss: 1.5950 - val_binary_crossentropy: 0.6447 - val_jaccard_coef_int: 0.5469\n",
      "Epoch 41/50\n",
      "208/214 [============================>.] - ETA: 0s - loss: 1.6304 - binary_crossentropy: 0.7752 - jaccard_coef_int: 0.5874Epoch 00041: loss improved from 1.62773 to 1.62439, saving model to unet.hdf5\n",
      "214/214 [==============================] - 41s 191ms/step - loss: 1.6244 - binary_crossentropy: 0.7720 - jaccard_coef_int: 0.5883 - val_loss: 1.6521 - val_binary_crossentropy: 0.7390 - val_jaccard_coef_int: 0.5584\n",
      "Epoch 42/50\n",
      "208/214 [============================>.] - ETA: 0s - loss: 1.5731 - binary_crossentropy: 0.7549 - jaccard_coef_int: 0.6043Epoch 00042: loss improved from 1.62439 to 1.55859, saving model to unet.hdf5\n",
      "214/214 [==============================] - 41s 191ms/step - loss: 1.5586 - binary_crossentropy: 0.7462 - jaccard_coef_int: 0.6063 - val_loss: 1.7353 - val_binary_crossentropy: 0.7488 - val_jaccard_coef_int: 0.5133\n",
      "Epoch 43/50\n",
      "208/214 [============================>.] - ETA: 0s - loss: 1.6898 - binary_crossentropy: 0.8200 - jaccard_coef_int: 0.5793Epoch 00043: loss did not improve\n",
      "214/214 [==============================] - 41s 190ms/step - loss: 1.6771 - binary_crossentropy: 0.8140 - jaccard_coef_int: 0.5830 - val_loss: 1.6166 - val_binary_crossentropy: 0.7658 - val_jaccard_coef_int: 0.5471\n",
      "Epoch 44/50\n",
      "208/214 [============================>.] - ETA: 0s - loss: 1.5154 - binary_crossentropy: 0.7409 - jaccard_coef_int: 0.6118Epoch 00044: loss improved from 1.55859 to 1.51844, saving model to unet.hdf5\n",
      "214/214 [==============================] - 41s 192ms/step - loss: 1.5184 - binary_crossentropy: 0.7420 - jaccard_coef_int: 0.6127 - val_loss: 1.8260 - val_binary_crossentropy: 0.7096 - val_jaccard_coef_int: 0.4887\n",
      "Epoch 45/50\n",
      "208/214 [============================>.] - ETA: 0s - loss: 1.4500 - binary_crossentropy: 0.6959 - jaccard_coef_int: 0.6273Epoch 00045: loss improved from 1.51844 to 1.48864, saving model to unet.hdf5\n",
      "214/214 [==============================] - 41s 190ms/step - loss: 1.4886 - binary_crossentropy: 0.7196 - jaccard_coef_int: 0.6221 - val_loss: 2.2657 - val_binary_crossentropy: 0.9524 - val_jaccard_coef_int: 0.4438\n",
      "Epoch 46/50\n",
      "208/214 [============================>.] - ETA: 0s - loss: 1.6058 - binary_crossentropy: 0.7831 - jaccard_coef_int: 0.5942Epoch 00046: loss did not improve\n",
      "214/214 [==============================] - 41s 190ms/step - loss: 1.6156 - binary_crossentropy: 0.7894 - jaccard_coef_int: 0.5927 - val_loss: 1.7123 - val_binary_crossentropy: 0.6140 - val_jaccard_coef_int: 0.5290\n",
      "Epoch 47/50\n",
      "208/214 [============================>.] - ETA: 0s - loss: 1.5294 - binary_crossentropy: 0.7472 - jaccard_coef_int: 0.6117Epoch 00047: loss did not improve\n",
      "214/214 [==============================] - 41s 190ms/step - loss: 1.5631 - binary_crossentropy: 0.7708 - jaccard_coef_int: 0.6081 - val_loss: 1.6973 - val_binary_crossentropy: 0.7347 - val_jaccard_coef_int: 0.5508\n",
      "Epoch 48/50\n",
      "208/214 [============================>.] - ETA: 0s - loss: 1.4904 - binary_crossentropy: 0.7269 - jaccard_coef_int: 0.6240Epoch 00048: loss did not improve\n",
      "214/214 [==============================] - 40s 189ms/step - loss: 1.4913 - binary_crossentropy: 0.7278 - jaccard_coef_int: 0.6234 - val_loss: 1.5575 - val_binary_crossentropy: 0.6735 - val_jaccard_coef_int: 0.5551\n",
      "Epoch 49/50\n",
      "208/214 [============================>.] - ETA: 0s - loss: 1.4806 - binary_crossentropy: 0.7191 - jaccard_coef_int: 0.6224Epoch 00049: loss improved from 1.48864 to 1.47608, saving model to unet.hdf5\n",
      "214/214 [==============================] - 42s 195ms/step - loss: 1.4761 - binary_crossentropy: 0.7155 - jaccard_coef_int: 0.6227 - val_loss: 3.9081 - val_binary_crossentropy: 2.2641 - val_jaccard_coef_int: 0.3401\n",
      "Epoch 50/50\n",
      "208/214 [============================>.] - ETA: 0s - loss: 1.4157 - binary_crossentropy: 0.6940 - jaccard_coef_int: 0.6394Epoch 00050: loss improved from 1.47608 to 1.41956, saving model to unet.hdf5\n",
      "214/214 [==============================] - 42s 194ms/step - loss: 1.4196 - binary_crossentropy: 0.6962 - jaccard_coef_int: 0.6386 - val_loss: 1.6084 - val_binary_crossentropy: 0.7337 - val_jaccard_coef_int: 0.5680\n",
      "predict test data\n",
      "693/693 [==============================] - 56s 81ms/step\n"
     ]
    }
   ],
   "source": [
    "myunet = myUnet('buildings')\n",
    "myunet.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data\n",
      "------------------------------\n",
      "load train images...\n",
      "------------------------------\n",
      "------------------------------\n",
      "load test images...\n",
      "------------------------------\n",
      "loading data done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:20: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\")`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:21: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(axis=1)`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:23: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\")`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:24: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(axis=1)`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:28: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\")`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:29: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(axis=1)`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\", kernel_initializer=\"he_normal\")`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:32: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(axis=1)`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:36: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\")`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:37: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(axis=1)`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:39: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\")`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:40: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(axis=1)`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:44: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\")`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:45: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(axis=1)`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:47: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\")`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:48: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(axis=1)`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:53: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\")`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:54: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(axis=1)`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:56: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\")`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:57: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(axis=1)`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:61: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/usr/lib/python3.5/site-packages/keras/legacy/layers.py:458: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:62: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\")`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:63: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(axis=1)`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:65: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\")`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:66: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(axis=1)`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:69: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:70: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\")`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:71: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(axis=1)`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:73: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\")`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:74: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(axis=1)`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:77: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:78: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\")`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:79: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(axis=1)`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:81: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\")`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:82: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(axis=1)`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:85: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:86: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\")`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:87: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(axis=1)`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:89: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\")`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:90: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(axis=1)`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:92: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(2, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\")`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:94: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(1, (1, 1), activation=\"sigmoid\")`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:96: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"co...)`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:57: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:59: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"me...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got unet\n",
      "Fitting model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:116: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 214 samples, validate on 92 samples\n",
      "Epoch 1/50\n",
      "208/214 [============================>.] - ETA: 1s - loss: 6.1086 - binary_crossentropy: 0.1665 - jaccard_coef_int: 0.0049Epoch 00001: loss improved from inf to 6.09507, saving model to unet.hdf5\n",
      "214/214 [==============================] - 64s 300ms/step - loss: 6.0951 - binary_crossentropy: 0.1637 - jaccard_coef_int: 0.0056 - val_loss: 6.6437 - val_binary_crossentropy: 0.1324 - val_jaccard_coef_int: 0.0962\n",
      "Epoch 2/50\n",
      "208/214 [============================>.] - ETA: 0s - loss: 4.5258 - binary_crossentropy: 0.0969 - jaccard_coef_int: 0.0357Epoch 00002: loss improved from 6.09507 to 4.46294, saving model to unet.hdf5\n",
      "214/214 [==============================] - 43s 200ms/step - loss: 4.4629 - binary_crossentropy: 0.0952 - jaccard_coef_int: 0.0404 - val_loss: 5.6077 - val_binary_crossentropy: 0.1749 - val_jaccard_coef_int: 0.0980\n",
      "Epoch 3/50\n",
      "208/214 [============================>.] - ETA: 0s - loss: 2.4074 - binary_crossentropy: 0.1086 - jaccard_coef_int: 0.1554Epoch 00003: loss improved from 4.46294 to 2.40762, saving model to unet.hdf5\n",
      "214/214 [==============================] - 43s 199ms/step - loss: 2.4076 - binary_crossentropy: 0.1102 - jaccard_coef_int: 0.1548 - val_loss: 2.6077 - val_binary_crossentropy: 0.1525 - val_jaccard_coef_int: 0.1797\n",
      "Epoch 4/50\n",
      "208/214 [============================>.] - ETA: 0s - loss: 2.0916 - binary_crossentropy: 0.1196 - jaccard_coef_int: 0.1863Epoch 00004: loss improved from 2.40762 to 2.09152, saving model to unet.hdf5\n",
      "214/214 [==============================] - 41s 191ms/step - loss: 2.0915 - binary_crossentropy: 0.1218 - jaccard_coef_int: 0.1870 - val_loss: 2.1911 - val_binary_crossentropy: 0.1299 - val_jaccard_coef_int: 0.2303\n",
      "Epoch 5/50\n",
      "208/214 [============================>.] - ETA: 0s - loss: 1.9438 - binary_crossentropy: 0.1345 - jaccard_coef_int: 0.2102Epoch 00005: loss improved from 2.09152 to 1.93354, saving model to unet.hdf5\n",
      "214/214 [==============================] - 42s 196ms/step - loss: 1.9335 - binary_crossentropy: 0.1334 - jaccard_coef_int: 0.2117 - val_loss: 2.0314 - val_binary_crossentropy: 0.1432 - val_jaccard_coef_int: 0.2509\n",
      "Epoch 6/50\n",
      "208/214 [============================>.] - ETA: 0s - loss: 1.8940 - binary_crossentropy: 0.1447 - jaccard_coef_int: 0.2211Epoch 00006: loss improved from 1.93354 to 1.89008, saving model to unet.hdf5\n",
      "214/214 [==============================] - 43s 200ms/step - loss: 1.8901 - binary_crossentropy: 0.1453 - jaccard_coef_int: 0.2224 - val_loss: 2.4178 - val_binary_crossentropy: 0.1750 - val_jaccard_coef_int: 0.2014\n",
      "Epoch 7/50\n",
      "208/214 [============================>.] - ETA: 0s - loss: 1.7422 - binary_crossentropy: 0.1476 - jaccard_coef_int: 0.2527Epoch 00007: loss improved from 1.89008 to 1.73617, saving model to unet.hdf5\n",
      "214/214 [==============================] - 43s 201ms/step - loss: 1.7362 - binary_crossentropy: 0.1470 - jaccard_coef_int: 0.2536 - val_loss: 1.9552 - val_binary_crossentropy: 0.1729 - val_jaccard_coef_int: 0.2615\n",
      "Epoch 8/50\n",
      "208/214 [============================>.] - ETA: 0s - loss: 1.7211 - binary_crossentropy: 0.1516 - jaccard_coef_int: 0.2548Epoch 00008: loss improved from 1.73617 to 1.71094, saving model to unet.hdf5\n",
      "214/214 [==============================] - 43s 199ms/step - loss: 1.7109 - binary_crossentropy: 0.1507 - jaccard_coef_int: 0.2569 - val_loss: 1.7541 - val_binary_crossentropy: 0.1556 - val_jaccard_coef_int: 0.3004\n",
      "Epoch 9/50\n",
      "208/214 [============================>.] - ETA: 0s - loss: 1.6520 - binary_crossentropy: 0.1508 - jaccard_coef_int: 0.2662Epoch 00009: loss improved from 1.71094 to 1.64955, saving model to unet.hdf5\n",
      "214/214 [==============================] - 43s 200ms/step - loss: 1.6496 - binary_crossentropy: 0.1508 - jaccard_coef_int: 0.2670 - val_loss: 1.8031 - val_binary_crossentropy: 0.1672 - val_jaccard_coef_int: 0.2937\n",
      "Epoch 10/50\n",
      "208/214 [============================>.] - ETA: 0s - loss: 1.5306 - binary_crossentropy: 0.1425 - jaccard_coef_int: 0.2962Epoch 00010: loss improved from 1.64955 to 1.52660, saving model to unet.hdf5\n",
      "214/214 [==============================] - 43s 199ms/step - loss: 1.5266 - binary_crossentropy: 0.1416 - jaccard_coef_int: 0.2967 - val_loss: 1.6699 - val_binary_crossentropy: 0.1585 - val_jaccard_coef_int: 0.3214\n",
      "Epoch 11/50\n",
      "208/214 [============================>.] - ETA: 0s - loss: 2.9671 - binary_crossentropy: 0.3186 - jaccard_coef_int: 0.1592Epoch 00011: loss did not improve\n",
      "214/214 [==============================] - 41s 193ms/step - loss: 2.9527 - binary_crossentropy: 0.3124 - jaccard_coef_int: 0.1583 - val_loss: 3.7220 - val_binary_crossentropy: 0.5727 - val_jaccard_coef_int: 0.1000\n",
      "Epoch 12/50\n",
      "208/214 [============================>.] - ETA: 0s - loss: 2.0555 - binary_crossentropy: 0.1540 - jaccard_coef_int: 0.1908Epoch 00012: loss did not improve\n",
      "214/214 [==============================] - 43s 200ms/step - loss: 2.0969 - binary_crossentropy: 0.1541 - jaccard_coef_int: 0.1870 - val_loss: 3.2886 - val_binary_crossentropy: 0.4977 - val_jaccard_coef_int: 0.1073\n",
      "Epoch 13/50\n",
      "208/214 [============================>.] - ETA: 0s - loss: 2.0230 - binary_crossentropy: 0.1718 - jaccard_coef_int: 0.1907Epoch 00013: loss did not improve\n",
      "214/214 [==============================] - 43s 199ms/step - loss: 2.0233 - binary_crossentropy: 0.1720 - jaccard_coef_int: 0.1904 - val_loss: 2.2242 - val_binary_crossentropy: 0.1910 - val_jaccard_coef_int: 0.2198\n",
      "Epoch 14/50\n",
      "208/214 [============================>.] - ETA: 0s - loss: 1.8949 - binary_crossentropy: 0.1747 - jaccard_coef_int: 0.2122Epoch 00014: loss did not improve\n",
      "214/214 [==============================] - 42s 197ms/step - loss: 1.8856 - binary_crossentropy: 0.1733 - jaccard_coef_int: 0.2139 - val_loss: 2.0532 - val_binary_crossentropy: 0.1844 - val_jaccard_coef_int: 0.2460\n",
      "Epoch 15/50\n",
      "208/214 [============================>.] - ETA: 0s - loss: 1.7625 - binary_crossentropy: 0.1738 - jaccard_coef_int: 0.2392Epoch 00015: loss did not improve\n",
      "214/214 [==============================] - 43s 200ms/step - loss: 1.7688 - binary_crossentropy: 0.1733 - jaccard_coef_int: 0.2375 - val_loss: 2.0901 - val_binary_crossentropy: 0.1879 - val_jaccard_coef_int: 0.2410\n",
      "Epoch 16/50\n",
      "208/214 [============================>.] - ETA: 0s - loss: 1.7147 - binary_crossentropy: 0.1817 - jaccard_coef_int: 0.2489Epoch 00016: loss did not improve\n",
      "214/214 [==============================] - 42s 198ms/step - loss: 1.7128 - binary_crossentropy: 0.1819 - jaccard_coef_int: 0.2495 - val_loss: 2.2032 - val_binary_crossentropy: 0.2099 - val_jaccard_coef_int: 0.2296\n",
      "Epoch 17/50\n",
      "208/214 [============================>.] - ETA: 0s - loss: 1.6367 - binary_crossentropy: 0.1787 - jaccard_coef_int: 0.2655Epoch 00017: loss did not improve\n",
      "214/214 [==============================] - 42s 197ms/step - loss: 1.6374 - binary_crossentropy: 0.1791 - jaccard_coef_int: 0.2658 - val_loss: 2.1351 - val_binary_crossentropy: 0.2151 - val_jaccard_coef_int: 0.2389\n",
      "Epoch 18/50\n",
      "208/214 [============================>.] - ETA: 0s - loss: 1.5686 - binary_crossentropy: 0.1771 - jaccard_coef_int: 0.2824Epoch 00018: loss did not improve\n",
      "214/214 [==============================] - 42s 194ms/step - loss: 1.5665 - binary_crossentropy: 0.1770 - jaccard_coef_int: 0.2829 - val_loss: 1.9085 - val_binary_crossentropy: 0.2099 - val_jaccard_coef_int: 0.2758\n",
      "Epoch 19/50\n",
      "208/214 [============================>.] - ETA: 0s - loss: 1.5349 - binary_crossentropy: 0.1758 - jaccard_coef_int: 0.2918Epoch 00019: loss did not improve\n",
      "214/214 [==============================] - 43s 200ms/step - loss: 1.5371 - binary_crossentropy: 0.1771 - jaccard_coef_int: 0.2915 - val_loss: 1.6946 - val_binary_crossentropy: 0.1979 - val_jaccard_coef_int: 0.3153\n",
      "Epoch 20/50\n",
      "208/214 [============================>.] - ETA: 0s - loss: 1.5185 - binary_crossentropy: 0.1810 - jaccard_coef_int: 0.2949Epoch 00020: loss improved from 1.52660 to 1.52284, saving model to unet.hdf5\n",
      "214/214 [==============================] - 42s 196ms/step - loss: 1.5228 - binary_crossentropy: 0.1821 - jaccard_coef_int: 0.2940 - val_loss: 1.9728 - val_binary_crossentropy: 0.2150 - val_jaccard_coef_int: 0.2675\n",
      "Epoch 21/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "208/214 [============================>.] - ETA: 0s - loss: 1.5838 - binary_crossentropy: 0.1918 - jaccard_coef_int: 0.2857Epoch 00021: loss did not improve\n",
      "214/214 [==============================] - 43s 199ms/step - loss: 1.5791 - binary_crossentropy: 0.1911 - jaccard_coef_int: 0.2865 - val_loss: 1.6778 - val_binary_crossentropy: 0.2003 - val_jaccard_coef_int: 0.3210\n",
      "Epoch 22/50\n",
      "208/214 [============================>.] - ETA: 0s - loss: 1.4863 - binary_crossentropy: 0.1769 - jaccard_coef_int: 0.3083Epoch 00022: loss improved from 1.52284 to 1.48195, saving model to unet.hdf5\n",
      "214/214 [==============================] - 43s 200ms/step - loss: 1.4819 - binary_crossentropy: 0.1771 - jaccard_coef_int: 0.3096 - val_loss: 1.6405 - val_binary_crossentropy: 0.2013 - val_jaccard_coef_int: 0.3238\n",
      "Epoch 23/50\n",
      "208/214 [============================>.] - ETA: 0s - loss: 1.4767 - binary_crossentropy: 0.1824 - jaccard_coef_int: 0.3111Epoch 00023: loss improved from 1.48195 to 1.47697, saving model to unet.hdf5\n",
      "214/214 [==============================] - 42s 197ms/step - loss: 1.4770 - binary_crossentropy: 0.1828 - jaccard_coef_int: 0.3108 - val_loss: 1.7202 - val_binary_crossentropy: 0.2076 - val_jaccard_coef_int: 0.3127\n",
      "Epoch 24/50\n",
      "208/214 [============================>.] - ETA: 0s - loss: 1.4112 - binary_crossentropy: 0.1760 - jaccard_coef_int: 0.3242Epoch 00024: loss improved from 1.47697 to 1.42135, saving model to unet.hdf5\n",
      "214/214 [==============================] - 43s 202ms/step - loss: 1.4214 - binary_crossentropy: 0.1777 - jaccard_coef_int: 0.3220 - val_loss: 1.5483 - val_binary_crossentropy: 0.2036 - val_jaccard_coef_int: 0.3500\n",
      "Epoch 25/50\n",
      "208/214 [============================>.] - ETA: 0s - loss: 1.3425 - binary_crossentropy: 0.1730 - jaccard_coef_int: 0.3469Epoch 00025: loss improved from 1.42135 to 1.35184, saving model to unet.hdf5\n",
      "214/214 [==============================] - 43s 200ms/step - loss: 1.3518 - binary_crossentropy: 0.1723 - jaccard_coef_int: 0.3440 - val_loss: 1.4889 - val_binary_crossentropy: 0.1840 - val_jaccard_coef_int: 0.3689\n",
      "Epoch 26/50\n",
      "208/214 [============================>.] - ETA: 0s - loss: 1.4082 - binary_crossentropy: 0.1807 - jaccard_coef_int: 0.3307Epoch 00026: loss did not improve\n",
      "214/214 [==============================] - 42s 197ms/step - loss: 1.4070 - binary_crossentropy: 0.1799 - jaccard_coef_int: 0.3304 - val_loss: 1.4668 - val_binary_crossentropy: 0.1840 - val_jaccard_coef_int: 0.3769\n",
      "Epoch 27/50\n",
      "208/214 [============================>.] - ETA: 0s - loss: 1.7873 - binary_crossentropy: 0.2385 - jaccard_coef_int: 0.2475Epoch 00027: loss did not improve\n",
      "214/214 [==============================] - 43s 200ms/step - loss: 1.7768 - binary_crossentropy: 0.2371 - jaccard_coef_int: 0.2496 - val_loss: 1.6923 - val_binary_crossentropy: 0.2198 - val_jaccard_coef_int: 0.3092\n",
      "Epoch 28/50\n",
      "208/214 [============================>.] - ETA: 0s - loss: 1.4990 - binary_crossentropy: 0.1869 - jaccard_coef_int: 0.2997Epoch 00028: loss did not improve\n",
      "214/214 [==============================] - 43s 200ms/step - loss: 1.4982 - binary_crossentropy: 0.1885 - jaccard_coef_int: 0.3007 - val_loss: 1.6445 - val_binary_crossentropy: 0.2013 - val_jaccard_coef_int: 0.3271\n",
      "Epoch 29/50\n",
      "208/214 [============================>.] - ETA: 0s - loss: 1.3727 - binary_crossentropy: 0.1794 - jaccard_coef_int: 0.3376Epoch 00029: loss did not improve\n",
      "214/214 [==============================] - 43s 199ms/step - loss: 1.3680 - binary_crossentropy: 0.1791 - jaccard_coef_int: 0.3392 - val_loss: 1.5999 - val_binary_crossentropy: 0.1922 - val_jaccard_coef_int: 0.3419\n",
      "Epoch 30/50\n",
      "208/214 [============================>.] - ETA: 0s - loss: 1.4124 - binary_crossentropy: 0.1854 - jaccard_coef_int: 0.3301Epoch 00030: loss did not improve\n",
      "214/214 [==============================] - 42s 198ms/step - loss: 1.4089 - binary_crossentropy: 0.1840 - jaccard_coef_int: 0.3304 - val_loss: 1.8407 - val_binary_crossentropy: 0.2030 - val_jaccard_coef_int: 0.3010\n",
      "Epoch 31/50\n",
      "208/214 [============================>.] - ETA: 0s - loss: 1.2932 - binary_crossentropy: 0.1681 - jaccard_coef_int: 0.3584Epoch 00031: loss improved from 1.35184 to 1.28997, saving model to unet.hdf5\n",
      "214/214 [==============================] - 42s 197ms/step - loss: 1.2900 - binary_crossentropy: 0.1690 - jaccard_coef_int: 0.3602 - val_loss: 1.4380 - val_binary_crossentropy: 0.1765 - val_jaccard_coef_int: 0.3867\n",
      "Epoch 32/50\n",
      "208/214 [============================>.] - ETA: 0s - loss: 1.2938 - binary_crossentropy: 0.1725 - jaccard_coef_int: 0.3616Epoch 00032: loss did not improve\n",
      "214/214 [==============================] - 42s 198ms/step - loss: 1.2924 - binary_crossentropy: 0.1717 - jaccard_coef_int: 0.3615 - val_loss: 1.4414 - val_binary_crossentropy: 0.1874 - val_jaccard_coef_int: 0.3851\n",
      "Epoch 33/50\n",
      "208/214 [============================>.] - ETA: 0s - loss: 1.2695 - binary_crossentropy: 0.1683 - jaccard_coef_int: 0.3660Epoch 00033: loss improved from 1.28997 to 1.26194, saving model to unet.hdf5\n",
      "214/214 [==============================] - 43s 202ms/step - loss: 1.2619 - binary_crossentropy: 0.1680 - jaccard_coef_int: 0.3689 - val_loss: 1.3940 - val_binary_crossentropy: 0.1847 - val_jaccard_coef_int: 0.3979\n",
      "Epoch 34/50\n",
      "208/214 [============================>.] - ETA: 0s - loss: 1.2687 - binary_crossentropy: 0.1733 - jaccard_coef_int: 0.3692Epoch 00034: loss did not improve\n",
      "214/214 [==============================] - 42s 196ms/step - loss: 1.2674 - binary_crossentropy: 0.1732 - jaccard_coef_int: 0.3695 - val_loss: 1.4190 - val_binary_crossentropy: 0.1907 - val_jaccard_coef_int: 0.3844\n",
      "Epoch 35/50\n",
      "208/214 [============================>.] - ETA: 0s - loss: 1.2426 - binary_crossentropy: 0.1717 - jaccard_coef_int: 0.3747Epoch 00035: loss improved from 1.26194 to 1.24988, saving model to unet.hdf5\n",
      "214/214 [==============================] - 43s 200ms/step - loss: 1.2499 - binary_crossentropy: 0.1727 - jaccard_coef_int: 0.3725 - val_loss: 1.5755 - val_binary_crossentropy: 0.2241 - val_jaccard_coef_int: 0.3295\n",
      "Epoch 36/50\n",
      "208/214 [============================>.] - ETA: 0s - loss: 1.2527 - binary_crossentropy: 0.1755 - jaccard_coef_int: 0.3730Epoch 00036: loss did not improve\n",
      "214/214 [==============================] - 43s 200ms/step - loss: 1.2577 - binary_crossentropy: 0.1723 - jaccard_coef_int: 0.3713 - val_loss: 1.4142 - val_binary_crossentropy: 0.1794 - val_jaccard_coef_int: 0.3978\n",
      "Epoch 37/50\n",
      "208/214 [============================>.] - ETA: 0s - loss: 1.2438 - binary_crossentropy: 0.1718 - jaccard_coef_int: 0.3755Epoch 00037: loss improved from 1.24988 to 1.24039, saving model to unet.hdf5\n",
      "214/214 [==============================] - 42s 197ms/step - loss: 1.2404 - binary_crossentropy: 0.1704 - jaccard_coef_int: 0.3760 - val_loss: 1.3743 - val_binary_crossentropy: 0.1815 - val_jaccard_coef_int: 0.4050\n",
      "Epoch 38/50\n",
      "208/214 [============================>.] - ETA: 0s - loss: 1.2207 - binary_crossentropy: 0.1640 - jaccard_coef_int: 0.3792Epoch 00038: loss improved from 1.24039 to 1.22097, saving model to unet.hdf5\n",
      "214/214 [==============================] - 43s 200ms/step - loss: 1.2210 - binary_crossentropy: 0.1669 - jaccard_coef_int: 0.3807 - val_loss: 1.3816 - val_binary_crossentropy: 0.1741 - val_jaccard_coef_int: 0.4094\n",
      "Epoch 39/50\n",
      "208/214 [============================>.] - ETA: 0s - loss: 1.1829 - binary_crossentropy: 0.1607 - jaccard_coef_int: 0.3919Epoch 00039: loss improved from 1.22097 to 1.19500, saving model to unet.hdf5\n",
      "214/214 [==============================] - 43s 201ms/step - loss: 1.1950 - binary_crossentropy: 0.1633 - jaccard_coef_int: 0.3887 - val_loss: 1.8036 - val_binary_crossentropy: 0.3034 - val_jaccard_coef_int: 0.2945\n",
      "Epoch 40/50\n",
      "208/214 [============================>.] - ETA: 0s - loss: 1.2067 - binary_crossentropy: 0.1709 - jaccard_coef_int: 0.3881Epoch 00040: loss did not improve\n",
      "214/214 [==============================] - 43s 202ms/step - loss: 1.2029 - binary_crossentropy: 0.1688 - jaccard_coef_int: 0.3890 - val_loss: 1.3570 - val_binary_crossentropy: 0.1854 - val_jaccard_coef_int: 0.4129\n",
      "Epoch 41/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "208/214 [============================>.] - ETA: 0s - loss: 1.1786 - binary_crossentropy: 0.1596 - jaccard_coef_int: 0.3921Epoch 00041: loss improved from 1.19500 to 1.17527, saving model to unet.hdf5\n",
      "214/214 [==============================] - 42s 198ms/step - loss: 1.1753 - binary_crossentropy: 0.1613 - jaccard_coef_int: 0.3949 - val_loss: 1.3795 - val_binary_crossentropy: 0.1839 - val_jaccard_coef_int: 0.4040\n",
      "Epoch 42/50\n",
      "208/214 [============================>.] - ETA: 0s - loss: 1.3009 - binary_crossentropy: 0.1789 - jaccard_coef_int: 0.3651Epoch 00042: loss did not improve\n",
      "214/214 [==============================] - 42s 198ms/step - loss: 1.2976 - binary_crossentropy: 0.1792 - jaccard_coef_int: 0.3662 - val_loss: 1.3892 - val_binary_crossentropy: 0.1824 - val_jaccard_coef_int: 0.4094\n",
      "Epoch 43/50\n",
      "208/214 [============================>.] - ETA: 0s - loss: 1.1583 - binary_crossentropy: 0.1612 - jaccard_coef_int: 0.3991Epoch 00043: loss improved from 1.17527 to 1.15490, saving model to unet.hdf5\n",
      "214/214 [==============================] - 43s 201ms/step - loss: 1.1549 - binary_crossentropy: 0.1618 - jaccard_coef_int: 0.4010 - val_loss: 1.3485 - val_binary_crossentropy: 0.1784 - val_jaccard_coef_int: 0.4186\n",
      "Epoch 44/50\n",
      "208/214 [============================>.] - ETA: 0s - loss: 1.1744 - binary_crossentropy: 0.1660 - jaccard_coef_int: 0.3970Epoch 00044: loss did not improve\n",
      "214/214 [==============================] - 43s 199ms/step - loss: 1.1782 - binary_crossentropy: 0.1668 - jaccard_coef_int: 0.3957 - val_loss: 1.3858 - val_binary_crossentropy: 0.1917 - val_jaccard_coef_int: 0.3936\n",
      "Epoch 45/50\n",
      "208/214 [============================>.] - ETA: 0s - loss: 1.1481 - binary_crossentropy: 0.1629 - jaccard_coef_int: 0.4033Epoch 00045: loss improved from 1.15490 to 1.14949, saving model to unet.hdf5\n",
      "214/214 [==============================] - 42s 196ms/step - loss: 1.1495 - binary_crossentropy: 0.1650 - jaccard_coef_int: 0.4040 - val_loss: 1.4310 - val_binary_crossentropy: 0.2008 - val_jaccard_coef_int: 0.3777\n",
      "Epoch 46/50\n",
      "208/214 [============================>.] - ETA: 0s - loss: 1.1444 - binary_crossentropy: 0.1589 - jaccard_coef_int: 0.4062Epoch 00046: loss improved from 1.14949 to 1.14442, saving model to unet.hdf5\n",
      "214/214 [==============================] - 43s 200ms/step - loss: 1.1444 - binary_crossentropy: 0.1594 - jaccard_coef_int: 0.4064 - val_loss: 1.3063 - val_binary_crossentropy: 0.1789 - val_jaccard_coef_int: 0.4327\n",
      "Epoch 47/50\n",
      "208/214 [============================>.] - ETA: 0s - loss: 1.1139 - binary_crossentropy: 0.1570 - jaccard_coef_int: 0.4146Epoch 00047: loss improved from 1.14442 to 1.11503, saving model to unet.hdf5\n",
      "214/214 [==============================] - 42s 197ms/step - loss: 1.1150 - binary_crossentropy: 0.1572 - jaccard_coef_int: 0.4141 - val_loss: 1.2852 - val_binary_crossentropy: 0.1773 - val_jaccard_coef_int: 0.4410\n",
      "Epoch 48/50\n",
      "208/214 [============================>.] - ETA: 0s - loss: 1.1264 - binary_crossentropy: 0.1655 - jaccard_coef_int: 0.4137Epoch 00048: loss did not improve\n",
      "214/214 [==============================] - 42s 194ms/step - loss: 1.1315 - binary_crossentropy: 0.1645 - jaccard_coef_int: 0.4121 - val_loss: 1.6698 - val_binary_crossentropy: 0.2645 - val_jaccard_coef_int: 0.3160\n",
      "Epoch 49/50\n",
      "208/214 [============================>.] - ETA: 0s - loss: 1.1691 - binary_crossentropy: 0.1650 - jaccard_coef_int: 0.3989Epoch 00049: loss did not improve\n",
      "214/214 [==============================] - 42s 195ms/step - loss: 1.1711 - binary_crossentropy: 0.1670 - jaccard_coef_int: 0.3990 - val_loss: 1.3250 - val_binary_crossentropy: 0.1777 - val_jaccard_coef_int: 0.4267\n",
      "Epoch 50/50\n",
      "208/214 [============================>.] - ETA: 0s - loss: 1.1099 - binary_crossentropy: 0.1566 - jaccard_coef_int: 0.4171Epoch 00050: loss improved from 1.11503 to 1.11052, saving model to unet.hdf5\n",
      "214/214 [==============================] - 43s 199ms/step - loss: 1.1105 - binary_crossentropy: 0.1556 - jaccard_coef_int: 0.4162 - val_loss: 1.3564 - val_binary_crossentropy: 0.1781 - val_jaccard_coef_int: 0.4239\n",
      "predict test data\n",
      "693/693 [==============================] - 57s 83ms/step\n"
     ]
    }
   ],
   "source": [
    "myunet = myUnet('cars')\n",
    "myunet.train()\n",
    "# myunet.save_img()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define the checkpoint\n",
    "# filepath=\"results/weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\"\n",
    "# checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "# callbacks_list = [checkpoint]\n",
    "\n",
    "# # fit the model\n",
    "# model.fit(X, y, epochs=50, batch_size=64, callbacks=callbacks_list)\n",
    "\n",
    "# # load the network weights\n",
    "# filename = \"results/weights-improvement-49-0.4748-bigger.hdf5\"\n",
    "# model.load_weights(filename)\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5",
   "language": "python",
   "name": "python3.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
